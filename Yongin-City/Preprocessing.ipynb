{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObmcyN1stCm6",
    "outputId": "fde659a1-4ba6-4ca9-8185-04cdb8b4dfe4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHkHmrYJXx36"
   },
   "source": [
    "# preprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klGQtHX4taov",
    "outputId": "c9116fff-ac05-4848-d108-47d30e7f3b42"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install emot\n",
    "!pip install contractions\n",
    "!pip install symspellpy\n",
    "!pip -q install evaluate\n",
    "!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5m3F-nXLtbkM",
    "outputId": "cd14ebce-909b-4344-d5e8-f933c6a07fa8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from emot.emo_unicode import UNICODE_EMOJI\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYUBjtdptm0G",
    "outputId": "38103e69-c0be-451b-8707-b4c73687413d"
   },
   "outputs": [],
   "source": [
    "# 런타임 확인\n",
    "n_devices = torch.cuda.device_count()\n",
    "print(n_devices)\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YZWFtOwtnxA"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/drive/MyDrive/용인시 SW 해커톤/translate_train.csv')\n",
    "test = pd.read_csv('/content/drive/MyDrive/용인시 SW 해커톤/translate_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "-swj93O--7Ax",
    "outputId": "fee2003d-62e1-47aa-a72e-72a9e73bfb86"
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4X8mteCtqSi",
    "outputId": "6581ae8f-9741-4728-8784-9f8e35b6c4c2"
   },
   "outputs": [],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIE_LvCLttAK",
    "outputId": "1e0cc0c7-4e2c-4c74-fb89-1b20f758374c"
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5_JRnWBvWeW",
    "outputId": "0cbd0322-f57a-4982-9b8d-8a5656202eed"
   },
   "outputs": [],
   "source": [
    "\"\"\"from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_english(text):\n",
    "    if text.isascii():\n",
    "        return text\n",
    "    else:\n",
    "        try:\n",
    "            translated_text = translator.translate(text, dest='en').text\n",
    "            return translated_text\n",
    "        except:\n",
    "            return text\n",
    "\n",
    "# test_data['text']에 있는 각 텍스트에 대해 번역을 적용합니다.\n",
    "for i, text in enumerate(test['text']):\n",
    "    test['text'][i] = translate_to_english(text)\n",
    "\n",
    "for i, text in enumerate(train['text']):\n",
    "    train['text'][i] = translate_to_english(text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBC1Y31N9RPz"
   },
   "outputs": [],
   "source": [
    "#train.to_csv('/content/drive/MyDrive/용인시 SW 해커톤/translate_train.csv', index=False)\n",
    "#test.to_csv('/content/drive/MyDrive/용인시 SW 해커톤/translate_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0vaOSmgtvxt"
   },
   "outputs": [],
   "source": [
    "# 1. Converitng Emojis\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "# 2. Lower Casing\n",
    "train[\"text\"] = train[\"text\"].str.lower()\n",
    "test[\"text\"] = test[\"text\"].str.lower()\n",
    "\n",
    "# 3. Removing HTML\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "# 4. Expand Contractions\n",
    "\n",
    "# 5. Removing URLs\n",
    "def remove_urls(text):\n",
    "    pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)(/\\w*)?')\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text\n",
    "\n",
    "# 6. Removing Email IDs\n",
    "def remove_emails(text):\n",
    "    pattern = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text\n",
    "\n",
    "# 7. Removing Tweeter Mentions\n",
    "def remove_mentions(text):\n",
    "    pattern = re.compile(r\"@\\w+\")\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 8. Removing Unicode Characters\n",
    "def remove_unicode_chars(text):\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    return text\n",
    "\n",
    "# 9. Abbreviation/Acronym Disambiguation\n",
    "def remove_abbreviations(text):\n",
    "    text = re.sub(r\"mh370\", \"missing malaysia airlines flight\", text)\n",
    "    text = re.sub(r\"okwx\", \"oklahoma city weather\", text)\n",
    "    text = re.sub(r\"arwx\", \"arkansas weather\", text)\n",
    "    text = re.sub(r\"gawx\", \"georgia weather\", text)\n",
    "    text = re.sub(r\"scwx\", \"south carolina weather\", text)\n",
    "    text = re.sub(r\"cawx\", \"california weather\", text)\n",
    "    text = re.sub(r\"tnwx\", \"tennessee weather\", text)\n",
    "    text = re.sub(r\"azwx\", \"arizona weather\", text)\n",
    "    text = re.sub(r\"alwx\", \"alabama Weather\", text)\n",
    "    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)\n",
    "    text = re.sub(r\"usnwsgov\", \"united states national weather service\", text)\n",
    "    text = re.sub(r\"suruc\", \"sanliurfa\", text)\n",
    "    return text\n",
    "\n",
    "# 10. Removing Punctuations\n",
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), \" \",text)\n",
    "    return text\n",
    "\n",
    "# 11. Removing Digits or Words with Digits\n",
    "def remove_digits(text):\n",
    "    pattern = re.compile(\"\\w*\\d+\\w*\")\n",
    "    text = re.sub(pattern, \"\",text)\n",
    "    return text\n",
    "\n",
    "# 12. Removing Extra Spaces\n",
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 13. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 14. Spelling Correction\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = \"/content/drive/MyDrive/용인시 SW 해커톤/frequency_dictionary_en_82_765.txt\"\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "def correct_spelling_symspell(text):\n",
    "    words = [\n",
    "        sym_spell.lookup(\n",
    "            word,\n",
    "            Verbosity.CLOSEST,\n",
    "            max_edit_distance=2,\n",
    "            include_unknown=True\n",
    "            )[0].term\n",
    "        for word in text.split()]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# 15. Correcting Compound Words\n",
    "\n",
    "bigram_path = \"/content/drive/MyDrive/용인시 SW 해커톤/frequency_bigramdictionary_en_243_342.txt\"\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "def correct_compound(text):\n",
    "    words = [\n",
    "        sym_spell.lookup_compound(\n",
    "            word,\n",
    "            max_edit_distance=2\n",
    "            )[0].term\n",
    "        for word in text.split()]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 16. Removing Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zs9i2DiOtw0n",
    "outputId": "f2bbd0e3-f556-4bb2-b583-195a193fa46d"
   },
   "outputs": [],
   "source": [
    "# 1. convert emojis\n",
    "train[\"text\"] = train[\"text\"].apply(convert_emojis)\n",
    "test[\"text\"] = test[\"text\"].apply(convert_emojis)\n",
    "\n",
    "# 2. lowercase\n",
    "train[\"text\"] = train[\"text\"].str.lower()\n",
    "test[\"text\"] = test[\"text\"].str.lower()\n",
    "\n",
    "# 3. remove html\n",
    "train[\"text\"] = train[\"text\"].apply(remove_html)\n",
    "test[\"text\"] = test[\"text\"].apply(remove_html)\n",
    "\n",
    "# 4. expand contractions\n",
    "# train[\"text\"] = train[\"text\"].apply(contractions.fix)\n",
    "# 5. remove urls\n",
    "train[\"text\"] = train[\"text\"].apply(remove_urls)\n",
    "test[\"text\"] = test[\"text\"].apply(remove_urls)\n",
    "\n",
    "# 6. remove e-mail ids\n",
    "train[\"text\"] = train[\"text\"].apply(remove_emails)\n",
    "test[\"text\"] = test[\"text\"].apply(remove_emails)\n",
    "\n",
    "# 7. remove mentions\n",
    "train[\"text\"] = train[\"text\"].apply(remove_mentions)\n",
    "test[\"text\"] = test[\"text\"].apply(remove_mentions)\n",
    "\n",
    "# 8. remove unicode chars\n",
    "train[\"text\"] = train[\"text\"].apply(remove_unicode_chars)\n",
    "test[\"text\"] = test[\"text\"].apply(remove_unicode_chars)\n",
    "\n",
    "# 9. remove abbreviations\n",
    "train[\"text\"] = train[\"text\"].apply(remove_abbreviations)\n",
    "test[\"text\"] = test[\"text\"].apply(remove_abbreviations)\n",
    "\n",
    "# 10. remove punctuations\n",
    "train[\"text\"] = train[\"text\"].apply(remove_punctuations)\n",
    "test[\"text\"] = test[\"text\"].apply(remove_punctuations)\n",
    "\n",
    "# 11. remove digits\n",
    "train[\"text\"] = train[\"text\"].apply(remove_digits)\n",
    "test[\"text\"] = test[\"text\"].apply(remove_digits)\n",
    "\n",
    "# 12. remove extra spaces\n",
    "train[\"text\"] = train[\"text\"].apply(remove_extra_spaces)\n",
    "test[\"text\"] = test[\"text\"].apply(remove_extra_spaces)\n",
    "\n",
    "# 13. lemmatization\n",
    "train[\"text\"] = train[\"text\"].apply(lemmatize_text)\n",
    "test[\"text\"] = test[\"text\"].apply(lemmatize_text)\n",
    "\n",
    "# 14. spelling correction\n",
    "train[\"text\"] = train[\"text\"].apply(correct_spelling_symspell)\n",
    "test[\"text\"] = test[\"text\"].apply(correct_spelling_symspell)\n",
    "\n",
    "# 15. correcting compound words\n",
    "train[\"text\"] = train[\"text\"].apply(correct_compound)\n",
    "test[\"text\"] = test[\"text\"].apply(correct_compound)\n",
    "\n",
    "# 16 remove stopwords\n",
    "#train[\"text\"] = train[\"text\"].apply(remove_stopwords)\n",
    "#test[\"text\"] = test[\"text\"].apply(remove_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XM3Bun6k_gst",
    "outputId": "19f7c7db-5758-481e-bd77-6502d6078503"
   },
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z9wmvPQ8_j9e",
    "outputId": "4641aa57-94e1-4aae-a0dc-73416ff4cf7b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 감정 사전 파일 불러오기\n",
    "sentiment_dict_path = \"/content/drive/MyDrive/용인시 SW 해커톤/sentiment_dict.csv\"\n",
    "sentiment_dict = pd.read_csv(sentiment_dict_path)\n",
    "\n",
    "# train 데이터프레임을 복사하여 새로운 열을 추가할 것입니다.\n",
    "train_with_sentiment_words = train.copy()\n",
    "\n",
    "# 'text' 열의 각 문장에 대해 sentiment_dict에 있는 단어를 찾아 sentiment_word 열에 추가\n",
    "def find_sentiment_words(text):\n",
    "    words = text.split()\n",
    "    sentiment_words = [word for word in words if word in sentiment_dict['word'].values]\n",
    "    return ' '.join(sentiment_words)\n",
    "\n",
    "train_with_sentiment_words['sentiment_word'] = train_with_sentiment_words['text'].apply(find_sentiment_words)\n",
    "\n",
    "# 결과 확인\n",
    "print(train_with_sentiment_words[['text', 'sentiment_word']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ManxK9NpA4y2",
    "outputId": "1a88454b-dba2-4323-f702-d8f7508d7df2"
   },
   "outputs": [],
   "source": [
    "train_with_sentiment_words.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "KpK5nfTtRqGS",
    "outputId": "ef13206e-3e38-48c5-a77a-5fcd63e74aac"
   },
   "outputs": [],
   "source": [
    "text_len = []\n",
    "for text in train.text:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len.append(tweet_len)\n",
    "\n",
    "train['text_len'] = text_len\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C13lsdBbaeQ6"
   },
   "outputs": [],
   "source": [
    "train = train[train['text_len'] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsrkG1LCSKR_",
    "outputId": "e20a7aed-3a82-45bf-d0f1-0a280dd6c063"
   },
   "outputs": [],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRKti-lbu4qV"
   },
   "outputs": [],
   "source": [
    "#train.to_csv('/content/drive/MyDrive/용인시 SW 해커톤/trans_pre_train.csv')\n",
    "test.to_csv('/content/drive/MyDrive/용인시 SW 해커톤/trans_pre_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKwZKKfxQ7XT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
